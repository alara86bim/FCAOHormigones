import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import io
import json
import time
import ssl
import urllib3
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
import requests
import os
from datetime import datetime

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Dashboard Control de Avance",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# T√≠tulo principal
st.title("üìä Dashboard Control de Avance - Hormigones, Moldajes y Enfierraduras")

def clean_private_key(private_key_str):
    """Limpia y formatea correctamente el private_key"""
    # Si ya tiene el formato correcto, devolverlo tal como est√°
    if "-----BEGIN PRIVATE KEY-----" in private_key_str and "-----END PRIVATE KEY-----" in private_key_str:
        # Reemplazar \\n por \n si es necesario
        cleaned_key = private_key_str.replace("\\n", "\n")
        return cleaned_key
    return private_key_str

# Configuraci√≥n de Google Drive
@st.cache_resource
def get_drive_service():
    """Obtiene el servicio de Google Drive usando las credenciales"""
    try:
        # Obtener las credenciales de los secretos
        creds_input = st.secrets["GOOGLE_CREDENTIALS"]
        
        # Debug: mostrar el tipo de datos recibido
        st.write(f"Tipo de credenciales recibidas: {type(creds_input)}")
        
        # Convertir a diccionario seg√∫n el tipo de entrada
        if isinstance(creds_input, str):
            # Si es string, intentar parsear como JSON
            try:
                creds_dict = json.loads(creds_input)
                st.success("‚úÖ Credenciales parseadas desde string JSON")
            except json.JSONDecodeError as e:
                st.error(f"‚ùå Error al parsear JSON: {e}")
                return None
        elif isinstance(creds_input, dict):
            # Si ya es un diccionario, usarlo directamente
            creds_dict = creds_input
            st.success("‚úÖ Credenciales recibidas como diccionario")
        else:
            st.error(f"‚ùå Tipo de credenciales no soportado: {type(creds_input)}")
            return None
        
        # Verificar que tenga los campos necesarios
        required_fields = ["type", "project_id", "private_key", "client_email"]
        missing_fields = [field for field in required_fields if field not in creds_dict]
        
        if missing_fields:
            st.error(f"‚ùå Faltan campos requeridos en las credenciales: {missing_fields}")
            return None
        
        # Crear una copia limpia de las credenciales
        clean_creds = {
            "type": creds_dict["type"],
            "project_id": creds_dict["project_id"],
            "private_key_id": creds_dict["private_key_id"],
            "client_email": creds_dict["client_email"],
            "client_id": creds_dict["client_id"],
            "auth_uri": creds_dict["auth_uri"],
            "token_uri": creds_dict["token_uri"],
            "auth_provider_x509_cert_url": creds_dict["auth_provider_x509_cert_url"],
            "client_x509_cert_url": creds_dict["client_x509_cert_url"]
        }
        
        # Manejar el private_key de forma especial
        private_key = creds_dict["private_key"]
        
        # Si el private_key tiene \\n, reemplazarlo por \n
        if "\\n" in private_key:
            private_key = private_key.replace("\\n", "\n")
            st.success("‚úÖ Private key con \\n convertido a \\n")
        
        clean_creds["private_key"] = private_key
        
        # Debug: mostrar las primeras l√≠neas del private_key
        st.write(f"Private key preview: {private_key[:100]}...")
        
        # Crear las credenciales
        creds = service_account.Credentials.from_service_account_info(
            clean_creds,
            scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        
        # Construir el servicio
        service = build('drive', 'v3', credentials=creds)
        
        # Probar la conexi√≥n
        try:
            service.files().list(pageSize=1).execute()
            st.success("‚úÖ Conexi√≥n con Google Drive exitosa")
            return service
        except Exception as e:
            st.error(f"‚ùå Error al conectar con Google Drive API: {e}")
            return None
            
    except Exception as e:
        st.error(f"‚ùå Error general al configurar Google Drive: {e}")
        st.error("Verifica que las credenciales est√©n en formato JSON v√°lido")
        return None

def download_file(service, file_id):
    """Lee directamente el contenido del archivo desde Google Drive"""
    try:
        # Leer el archivo directamente desde Google Drive
        request = service.files().get_media(fileId=file_id)
        file_content = request.execute()
        
        # Convertir el contenido a StringIO para que pandas pueda leerlo
        from io import StringIO
        content_str = file_content.decode('utf-8')
        return StringIO(content_str)
        
    except Exception as e:
        st.error(f"Error leyendo archivo desde Google Drive: {e}")
        return None

def list_files_in_folder(service, folder_id):
    """Lista archivos en una carpeta de Google Drive"""
    try:
        results = service.files().list(
            q=f"'{folder_id}' in parents",
            pageSize=1000,
            fields="files(id, name, modifiedTime)"
        ).execute()
        return results.get('files', [])
    except Exception as e:
        st.error(f"Error al listar archivos: {e}")
        return []

# Cargar datos desde Google Drive
@st.cache_data(ttl=3600)  # Cache por 1 hora
def cargar_datos():
    """Carga el archivo AO_GENERAL.txt desde Google Drive o local como fallback"""
    service = get_drive_service()
    
    # Intentar cargar desde Google Drive primero
    if service:
        try:
            file_id = st.secrets["FILE_ID_GENERAL"]
            fh = download_file(service, file_id)
            if fh:
                df = pd.read_csv(fh, sep="\t", header=1, dtype=str)
                df = df.dropna(how="all")
                
                # Limpiar columnas
                df = df.rename(columns=lambda x: x.strip().replace('"', ''))
                
                # Convertir tipos de datos
                for col in ["VolumenHA", "AreaMoldaje", "Cuantia"]:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col].str.replace(",", ".", regex=False), errors='coerce')
                
                if "FC_CON_FECHA EJECUCION" in df.columns:
                    df["FC_CON_FECHA EJECUCION"] = pd.to_datetime(
                        df["FC_CON_FECHA EJECUCION"], 
                        dayfirst=True, 
                        errors='coerce'
                    )
                
                st.success("‚úÖ Datos cargados desde Google Drive")
                return df
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error al cargar desde Google Drive: {e}")
    
    # Fallback: intentar cargar desde archivo local
    try:
        local_file = "AO_GENERAL.txt"
        if os.path.exists(local_file):
            df = pd.read_csv(local_file, sep="\t", header=1, dtype=str)
            df = df.dropna(how="all")
            
            # Limpiar columnas
            df = df.rename(columns=lambda x: x.strip().replace('"', ''))
            
            # Convertir tipos de datos
            for col in ["VolumenHA", "AreaMoldaje", "Cuantia"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col].str.replace(",", ".", regex=False), errors='coerce')
            
            if "FC_CON_FECHA EJECUCION" in df.columns:
                df["FC_CON_FECHA EJECUCION"] = pd.to_datetime(
                    df["FC_CON_FECHA EJECUCION"], 
                    dayfirst=True, 
                    errors='coerce'
                )
            
            st.success("‚úÖ Datos cargados desde archivo local (fallback)")
            return df
        else:
            st.error("‚ùå No se encontr√≥ el archivo local AO_GENERAL.txt")
    except Exception as e:
        st.error(f"‚ùå Error al cargar archivo local: {e}")
    
    return None

@st.cache_data(ttl=3600)  # Cache por 1 hora
def cargar_archivos_semanales():
    """Carga archivos semanales desde Google Drive o local como fallback"""
    service = get_drive_service()
    
    # Intentar cargar desde Google Drive primero
    if service:
        try:
            folder_id = st.secrets["FOLDER_ID_SEMANAL"]
            files = list_files_in_folder(service, folder_id)
            
            # Filtrar solo archivos *_AO_GENERAL.txt
            archivos = [f for f in files if f['name'].endswith('_AO_GENERAL.txt')]
            
            def extraer_fecha(nombre):
                # Intentar diferentes formatos de fecha
                patterns = [
                    r"(\d{2}-\d{2}-\d{4})_AO_GENERAL\.txt",  # DD-MM-YYYY
                    r"(\d{2}-\d{2}-\d{2})_AO_GENERAL\.txt",  # DD-MM-YY
                    r"(\d{4}-\d{2}-\d{2})_AO_GENERAL\.txt",  # YYYY-MM-DD
                ]
                
                for pattern in patterns:
                    m = re.match(pattern, nombre)
                    if m:
                        fecha_str = m.group(1)
                        try:
                            # Intentar diferentes formatos de fecha
                            if len(fecha_str.split('-')[2]) == 4:  # YYYY
                                if len(fecha_str.split('-')[0]) == 2:  # DD-MM-YYYY
                                    return pd.to_datetime(fecha_str, format='%d-%m-%Y', dayfirst=True)
                                else:  # YYYY-MM-DD
                                    return pd.to_datetime(fecha_str, format='%Y-%m-%d')
                            else:  # DD-MM-YY
                                return pd.to_datetime(fecha_str, format='%d-%m-%y', dayfirst=True)
                        except:
                            continue
                return None
            
            archivos_fechas = [(f, extraer_fecha(f['name'])) for f in archivos]
            archivos_fechas = sorted(
                [x for x in archivos_fechas if x[1] is not None], 
                key=lambda x: x[1]
            )
            
            if archivos_fechas:
                st.success("‚úÖ Archivos semanales cargados desde Google Drive")
                return archivos_fechas, service
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error al cargar archivos semanales desde Google Drive: {e}")
    
    # Fallback: intentar cargar desde carpeta local
    try:
        local_folder = "REPORTE SEMANAL"
        if os.path.exists(local_folder):
            archivos = []
            for filename in os.listdir(local_folder):
                if filename.endswith('_AO_GENERAL.txt'):
                    file_path = os.path.join(local_folder, filename)
                    # Crear un objeto similar al de Google Drive
                    file_obj = {
                        'id': file_path,  # Usar path como ID
                        'name': filename,
                        'local_path': file_path
                    }
                    archivos.append(file_obj)
            
            def extraer_fecha(nombre):
                # Intentar diferentes formatos de fecha
                patterns = [
                    r"(\d{2}-\d{2}-\d{4})_AO_GENERAL\.txt",  # DD-MM-YYYY
                    r"(\d{2}-\d{2}-\d{2})_AO_GENERAL\.txt",  # DD-MM-YY
                    r"(\d{4}-\d{2}-\d{2})_AO_GENERAL\.txt",  # YYYY-MM-DD
                ]
                
                for pattern in patterns:
                    m = re.match(pattern, nombre)
                    if m:
                        fecha_str = m.group(1)
                        try:
                            # Intentar diferentes formatos de fecha
                            if len(fecha_str.split('-')[2]) == 4:  # YYYY
                                if len(fecha_str.split('-')[0]) == 2:  # DD-MM-YYYY
                                    return pd.to_datetime(fecha_str, format='%d-%m-%Y', dayfirst=True)
                                else:  # YYYY-MM-DD
                                    return pd.to_datetime(fecha_str, format='%Y-%m-%d')
                            else:  # DD-MM-YY
                                return pd.to_datetime(fecha_str, format='%d-%m-%y', dayfirst=True)
                        except:
                            continue
                return None
            
            archivos_fechas = [(f, extraer_fecha(f['name'])) for f in archivos]
            archivos_fechas = sorted(
                [x for x in archivos_fechas if x[1] is not None], 
                key=lambda x: x[1]
            )
            
            if archivos_fechas:
                st.success("‚úÖ Archivos semanales cargados desde carpeta local (fallback)")
                return archivos_fechas, None  # None indica que es local
        else:
            st.warning("‚ö†Ô∏è No se encontr√≥ la carpeta local REPORTE SEMANAL")
    except Exception as e:
        st.error(f"‚ùå Error al cargar archivos locales: {e}")
    
    return [], None

def crear_tabla_interactiva(df, titulo, columna_volumen="VolumenHA", tab_key=""):
    """Crea una tabla interactiva con AgGrid, jerarqu√≠a expandible por Nivel y Elementos como matriz, mostrando tambi√©n las columnas Nivel y Elementos y los valores de VolumenHA, AreaMoldaje, Cuantia con 2 decimales si existen. El resumen general muestra los totales y el % de avance (Si/Total*100)."""
    from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode

    if df.empty:
        st.warning("No hay datos para mostrar")
        return

    # Determinar el par√°metro booleano seg√∫n el tipo de tabla
    if "Hormigon" in titulo or "Hormigon" in columna_volumen:
        param_bool = "Hormigonado"
    elif "Moldaje" in titulo or "Moldaje" in columna_volumen:
        param_bool = "Moldaje"
    elif "Enfierradura" in titulo or "Enfierradura" in columna_volumen:
        param_bool = "Enfierradura"
    else:
        st.error("No se pudo determinar el par√°metro booleano para esta tabla.")
        return

    # Verificar columnas necesarias
    required_columns = ["Nivel", "Elementos", param_bool]
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"Faltan las siguientes columnas en los datos: {', '.join(missing_columns)}")
        st.info("Columnas disponibles: " + ", ".join(df.columns.tolist()))
        return

    # Filtrar filas v√°lidas
    df_filtrado = df[df["Nivel"].notna() & (df["Nivel"].astype(str).str.strip() != "")]
    df_filtrado = df_filtrado[df_filtrado["Elementos"].notna() & (df_filtrado["Elementos"].astype(str).str.strip() != "")]
    if df_filtrado.empty:
        st.warning("No hay datos con niveles y elementos v√°lidos")
        return

    # Normalizar valores del par√°metro booleano
    df_filtrado[param_bool] = df_filtrado[param_bool].astype(str).str.strip().str.lower()
    # Considerar 's√≠' como positivo, el resto como 'no'
    df_filtrado["Es_Si"] = np.where(df_filtrado[param_bool].isin(["si", "s√≠", "true", "1"]), 1, 0)
    df_filtrado["Es_No"] = 1 - df_filtrado["Es_Si"]

    # Agrupar por Nivel y Elementos
    resumen = df_filtrado.groupby(["Nivel", "Elementos"]).agg(
        Si=("Es_Si", "sum"),
        No=("Es_No", "sum"),
        VolumenHA=("VolumenHA", "sum") if "VolumenHA" in df_filtrado.columns else (lambda x: np.nan),
        AreaMoldaje=("AreaMoldaje", "sum") if "AreaMoldaje" in df_filtrado.columns else (lambda x: np.nan),
        Cuantia=("Cuantia", "sum") if "Cuantia" in df_filtrado.columns else (lambda x: np.nan),
    ).reset_index()
    resumen["Total"] = resumen["Si"] + resumen["No"]
    resumen["Si%"] = (resumen["Si"] / resumen["Total"] * 100).round(2)
    resumen["No%"] = (resumen["No"] / resumen["Total"] * 100).round(2)

    # Formatear VolumenHA, AreaMoldaje, Cuantia a 2 decimales si existen
    for col in ["VolumenHA", "AreaMoldaje", "Cuantia"]:
        if col in resumen.columns:
            resumen[col] = resumen[col].astype(float).round(2)

    st.subheader(titulo)

    # Filtros
    col1, col2 = st.columns(2)
    with col1:
        niveles = ["Todos"] + sorted(resumen["Nivel"].unique())
        nivel_seleccionado = st.selectbox("Filtrar por Nivel:", niveles, key=f"nivel_{tab_key}")
    with col2:
        elementos = ["Todos"] + sorted(resumen["Elementos"].unique())
        elemento_seleccionado = st.selectbox("Filtrar por Elemento:", elementos, key=f"elemento_{tab_key}")

    df_filtrado_tabla = resumen.copy()
    if nivel_seleccionado != "Todos":
        df_filtrado_tabla = df_filtrado_tabla[df_filtrado_tabla["Nivel"] == nivel_seleccionado]
    if elemento_seleccionado != "Todos":
        df_filtrado_tabla = df_filtrado_tabla[df_filtrado_tabla["Elementos"] == elemento_seleccionado]

    # Configurar AgGrid para jerarqu√≠a expandible y columnas visibles
    gb = GridOptionsBuilder.from_dataframe(df_filtrado_tabla)
    gb.configure_default_column(resizable=True, filterable=True, sortable=True, editable=False)
    gb.configure_column("Nivel", rowGroup=True, rowGroupIndex=0)  # visible
    gb.configure_column("Elementos", rowGroup=True, rowGroupIndex=1)  # visible
    gb.configure_column("Si", type=["numericColumn", "numberColumnFilter"], width=80, valueFormatter="value.toFixed(0)")
    gb.configure_column("Si%", type=["numericColumn", "numberColumnFilter"], width=100, valueFormatter="value.toFixed(2) + '%'", cellStyle={"color": "green"})
    gb.configure_column("No", type=["numericColumn", "numberColumnFilter"], width=80, valueFormatter="value.toFixed(0)")
    gb.configure_column("No%", type=["numericColumn", "numberColumnFilter"], width=100, valueFormatter="value.toFixed(2) + '%'", cellStyle={"color": "red"})
    gb.configure_column("Total", type=["numericColumn", "numberColumnFilter"], width=100, valueFormatter="value.toFixed(0)")
    if "VolumenHA" in df_filtrado_tabla.columns:
        gb.configure_column("VolumenHA", type=["numericColumn", "numberColumnFilter"], width=120, valueFormatter="value.toFixed(2)")
    if "AreaMoldaje" in df_filtrado_tabla.columns:
        gb.configure_column("AreaMoldaje", type=["numericColumn", "numberColumnFilter"], width=120, valueFormatter="value.toFixed(2)")
    if "Cuantia" in df_filtrado_tabla.columns:
        gb.configure_column("Cuantia", type=["numericColumn", "numberColumnFilter"], width=120, valueFormatter="value.toFixed(2)")
    gb.configure_grid_options(
        domLayout='normal',
        enableRangeSelection=True,
        enableCharts=True,
        groupDisplayType='groupRows',
        groupDefaultExpanded=0  # Colapsado por defecto
    )
    grid_options = gb.build()

    AgGrid(
        df_filtrado_tabla,
        gridOptions=grid_options,
        data_return_mode=DataReturnMode.FILTERED_AND_SORTED,
        update_mode=GridUpdateMode.GRID_CHANGED,
        fit_columns_on_grid_load=True,
        theme='streamlit',
        height=400,
        allow_unsafe_jscode=True
    )

    # M√©tricas generales
    if not df_filtrado_tabla.empty:
        st.subheader("üìä Resumen General")
        cols = st.columns(7)
        with cols[0]:
            st.metric("Total Elementos", int(df_filtrado_tabla["Total"].sum()))
        with cols[1]:
            st.metric("Completados (S√≠)", int(df_filtrado_tabla["Si"].sum()))
        with cols[2]:
            st.metric("Pendientes (No)", int(df_filtrado_tabla["No"].sum()))
        with cols[3]:
            if "VolumenHA" in df_filtrado_tabla.columns:
                st.metric("VolumenHA", f"{df_filtrado_tabla['VolumenHA'].sum():.2f}")
        with cols[4]:
            if "AreaMoldaje" in df_filtrado_tabla.columns:
                st.metric("AreaMoldaje", f"{df_filtrado_tabla['AreaMoldaje'].sum():.2f}")
        with cols[5]:
            if "Cuantia" in df_filtrado_tabla.columns:
                st.metric("Cuantia", f"{df_filtrado_tabla['Cuantia'].sum():.2f}")
        with cols[6]:
            total = df_filtrado_tabla["Total"].sum()
            si = df_filtrado_tabla["Si"].sum()
            if total > 0:
                st.metric("% Avance", f"{(si/total*100):.2f}%")
            else:
                st.metric("% Avance", "0.00%")

def mostrar_avance_semanal(use_local_files=False):
    """Muestra el avance semanal de hormigones"""
    if use_local_files:
        archivos_fechas, service = cargar_archivos_semanales_local()
    else:
        archivos_fechas, service = cargar_archivos_semanales()
    
    if not archivos_fechas:
        st.warning("No se encontraron archivos semanales")
        return
    
    # Procesar archivos
    lista_df = []
    for f, fecha in archivos_fechas:
        try:
            if use_local_files:
                # Para archivos locales, f es el nombre del archivo
                filepath = os.path.join("REPORTE SEMANAL", f)
                fh = leer_archivo_local(filepath)
            else:
                # Para archivos de Google Drive, f es el objeto del archivo
                fh = download_file(service, f['id'])
            
            if not fh:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"No se pudo leer el archivo: {nombre_archivo}")
                continue
                
            # Leer el archivo con el formato correcto
            dfw = pd.read_csv(fh, sep='\t', header=1, dtype=str, quoting=3)  # QUOTE_NONE
            dfw = dfw.dropna(how="all")
            
            # Limpiar columnas (remover comillas)
            dfw = dfw.rename(columns=lambda x: x.strip().replace('"', '') if isinstance(x, str) else x)
            
            # Limpiar datos (remover comillas de los valores)
            for col in dfw.columns:
                if dfw[col].dtype == 'object':
                    dfw[col] = dfw[col].astype(str).str.replace('"', '')
            
            # Verificar que exista la columna VolumenHA
            if "VolumenHA" not in dfw.columns:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"Archivo {nombre_archivo} no tiene columna VolumenHA. Columnas disponibles: {list(dfw.columns)}")
                continue
            
            # Convertir VolumenHA a num√©rico
            dfw["VolumenHA"] = pd.to_numeric(
                dfw["VolumenHA"].str.replace(",", ".", regex=False), 
                errors='coerce'
            )
            
            # Solo Hormigonado = 'S√≠'
            if "Hormigonado" in dfw.columns:
                dfw = dfw[dfw["Hormigonado"] == "S√≠"]
            else:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"Archivo {nombre_archivo} no tiene columna Hormigonado")
                continue
            
            # Solo filas con Nivel y Elementos v√°lidos
            if "Nivel" in dfw.columns and "Elementos" in dfw.columns:
                dfw = dfw[dfw["Nivel"].notna() & (dfw["Nivel"].astype(str).str.strip() != "")]
                dfw = dfw[dfw["Elementos"].notna() & (dfw["Elementos"].astype(str).str.strip() != "")]
            else:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"Archivo {nombre_archivo} no tiene columnas Nivel o Elementos")
                continue
            
            # Agrupar por Nivel y Elementos
            resumen = dfw.groupby(["Nivel", "Elementos"])["VolumenHA"].sum().reset_index()
            resumen["Fecha"] = fecha
            lista_df.append(resumen)
            
        except Exception as e:
            nombre_archivo = f if use_local_files else f['name']
            st.error(f"Error procesando archivo {nombre_archivo}: {e}")
            continue
    
    if not lista_df:
        st.warning("No hay datos de hormigones para mostrar")
        return
    
    # Unir todos los resultados
    df_semana = pd.concat(lista_df, ignore_index=True)
    
    # Crear tabla pivot para comparaci√≥n
    pivot_semanal = df_semana.pivot_table(
        values="VolumenHA",
        index=["Nivel", "Elementos"],
        columns="Fecha",
        aggfunc="sum",
        fill_value=0
    ).reset_index()
    
    # Calcular diferencias entre semanas
    fechas = sorted(df_semana["Fecha"].unique())
    if len(fechas) >= 2:
        for i in range(1, len(fechas)):
            col_actual = fechas[i]
            col_anterior = fechas[i-1]
            if col_actual in pivot_semanal.columns and col_anterior in pivot_semanal.columns:
                pivot_semanal[f"Dif_{col_anterior.strftime('%d/%m')}_{col_actual.strftime('%d/%m')}"] = (
                    pivot_semanal[col_actual] - pivot_semanal[col_anterior]
                )
    
    # Calcular totales
    numeric_cols = pivot_semanal.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        pivot_semanal["Total"] = pivot_semanal[numeric_cols].sum(axis=1)
        if pivot_semanal["Total"].sum() > 0:
            pivot_semanal["% Avance"] = (pivot_semanal["Total"] / pivot_semanal["Total"].sum() * 100).round(2)
    
    # Formatear columnas num√©ricas
    for col in pivot_semanal.select_dtypes(include=[np.number]).columns:
        pivot_semanal[col] = pivot_semanal[col].round(2)
    
    # Ordenar por Nivel y Elemento para jerarqu√≠a visual
    pivot_semanal = pivot_semanal.sort_values(["Nivel", "Elementos"]).reset_index(drop=True)
    
    # Mostrar tabla
    st.subheader("Avance Semanal Hormigones")
    
    # Agregar filtros
    col1, col2 = st.columns(2)
    
    with col1:
        try:
            niveles_raw = df_semana["Nivel"].dropna().unique()
            niveles_clean = [str(n).strip() for n in niveles_raw if str(n).strip()]
            niveles = ["Todos"] + sorted(niveles_clean)
            nivel_seleccionado = st.selectbox("Filtrar por Nivel:", niveles, key="semanal_nivel")
        except Exception as e:
            st.error(f"Error al cargar niveles: {e}")
            nivel_seleccionado = "Todos"
    
    with col2:
        try:
            elementos_raw = df_semana["Elementos"].dropna().unique()
            elementos_clean = [str(e).strip() for e in elementos_raw if str(e).strip()]
            elementos = ["Todos"] + sorted(elementos_clean)
            elemento_seleccionado = st.selectbox("Filtrar por Elemento:", elementos, key="semanal_elemento")
        except Exception as e:
            st.error(f"Error al cargar elementos: {e}")
            elemento_seleccionado = "Todos"
    
    # Aplicar filtros
    df_filtrado_tabla = pivot_semanal.copy()
    try:
        if nivel_seleccionado != "Todos":
            df_filtrado_tabla = df_filtrado_tabla[df_filtrado_tabla["Nivel"] == nivel_seleccionado]
        if elemento_seleccionado != "Todos":
            df_filtrado_tabla = df_filtrado_tabla[df_filtrado_tabla["Elementos"] == elemento_seleccionado]
    except Exception as e:
        st.error(f"Error al aplicar filtros: {e}")
    
    # Mostrar tabla con jerarqu√≠as expandibles
    try:
        # Crear configuraci√≥n de columnas din√°mica
        column_config = {
            "Nivel": st.column_config.TextColumn("Nivel", width="medium"),
            "Elementos": st.column_config.TextColumn("Elementos", width="large"),
        }
        
        # Agregar columnas de fechas
        for col in df_filtrado_tabla.columns:
            if col not in ["Nivel", "Elementos", "Total", "% Avance"]:
                if "Dif_" in col:
                    column_config[col] = st.column_config.NumberColumn(col, format="%.2f")
                else:
                    column_config[col] = st.column_config.NumberColumn(col, format="%.2f")
        
        # Agregar columnas de totales
        if "Total" in df_filtrado_tabla.columns:
            column_config["Total"] = st.column_config.NumberColumn("Total", format="%.2f")
        if "% Avance" in df_filtrado_tabla.columns:
            column_config["% Avance"] = st.column_config.NumberColumn("% Avance", format="%.2f%%")
        
        st.dataframe(
            df_filtrado_tabla,
            use_container_width=True,
            hide_index=True,
            column_config=column_config
        )
    except Exception as e:
        st.error(f"Error al mostrar tabla: {e}")
        st.dataframe(df_filtrado_tabla, use_container_width=True)
    
    # Mostrar m√©tricas
    if not df_filtrado_tabla.empty and "Total" in df_filtrado_tabla.columns:
        try:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                total_sum = df_filtrado_tabla['Total'].sum()
                st.metric("Total General", f"{total_sum:.2f}")
            
            with col2:
                if len(fechas) >= 2:
                    ultima_fecha = fechas[-1]
                    if ultima_fecha in df_filtrado_tabla.columns:
                        ultimo_total = df_filtrado_tabla[ultima_fecha].sum()
                        st.metric(f"Total {ultima_fecha.strftime('%d/%m/%Y')}", f"{ultimo_total:.2f}")
            
            with col3:
                if len(fechas) >= 2:
                    primera_fecha = fechas[0]
                    if primera_fecha in df_filtrado_tabla.columns:
                        primer_total = df_filtrado_tabla[primera_fecha].sum()
                        st.metric(f"Total {primera_fecha.strftime('%d/%m/%Y')}", f"{primer_total:.2f}")
                        
        except Exception as e:
            st.error(f"Error al calcular m√©tricas: {e}")
    
    # Mostrar gr√°fico de tendencia
    if len(fechas) >= 2:
        st.subheader("Tendencia Semanal")
        df_tendencia = df_semana.groupby("Fecha")["VolumenHA"].sum().reset_index()
        fig = px.line(df_tendencia, x="Fecha", y="VolumenHA", 
                     title="Evoluci√≥n del Volumen de Hormig√≥n por Semana")
        st.plotly_chart(fig, use_container_width=True)

def mostrar_trisemanal(use_local_files=False):
    """Muestra comparaci√≥n trisemanal"""
    if use_local_files:
        archivos_fechas, service = cargar_archivos_semanales_local()
    else:
        archivos_fechas, service = cargar_archivos_semanales()
    
    if len(archivos_fechas) < 2:
        st.warning("Se necesitan al menos 2 archivos semanales para la comparaci√≥n trisemanal")
        return
    
    # Tomar solo los 2 √∫ltimos archivos
    archivos_fechas = archivos_fechas[-2:]
    
    # Procesar archivos
    lista_df = []
    for f, fecha in archivos_fechas:
        try:
            if use_local_files:
                # Para archivos locales, f es el nombre del archivo
                filepath = os.path.join("REPORTE SEMANAL", f)
                fh = leer_archivo_local(filepath)
            else:
                # Para archivos de Google Drive, f es el objeto del archivo
                fh = download_file(service, f['id'])
            
            if not fh:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"No se pudo leer el archivo: {nombre_archivo}")
                continue
                
            # Leer el archivo con el formato correcto
            dfw = pd.read_csv(fh, sep='\t', header=1, dtype=str, quoting=3)  # QUOTE_NONE
            dfw = dfw.dropna(how="all")
            
            # Limpiar columnas (remover comillas)
            dfw = dfw.rename(columns=lambda x: x.strip().replace('"', '') if isinstance(x, str) else x)
            
            # Limpiar datos (remover comillas de los valores)
            for col in dfw.columns:
                if dfw[col].dtype == 'object':
                    dfw[col] = dfw[col].astype(str).str.replace('"', '')
            
            # Verificar que exista la columna VolumenHA
            if "VolumenHA" not in dfw.columns:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"Archivo {nombre_archivo} no tiene columna VolumenHA. Columnas disponibles: {list(dfw.columns)}")
                continue
            
            # Convertir VolumenHA a num√©rico
            dfw["VolumenHA"] = pd.to_numeric(
                dfw["VolumenHA"].str.replace(",", ".", regex=False), 
                errors='coerce'
            )
            
            # Solo Hormigonado = 'S√≠'
            if "Hormigonado" in dfw.columns:
                dfw = dfw[dfw["Hormigonado"] == "S√≠"]
            else:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"Archivo {nombre_archivo} no tiene columna Hormigonado")
                continue
            
            # Solo filas con Nivel y Elementos v√°lidos
            if "Nivel" in dfw.columns and "Elementos" in dfw.columns:
                dfw = dfw[dfw["Nivel"].notna() & (dfw["Nivel"].astype(str).str.strip() != "")]
                dfw = dfw[dfw["Elementos"].notna() & (dfw["Elementos"].astype(str).str.strip() != "")]
            else:
                nombre_archivo = f if use_local_files else f['name']
                st.warning(f"Archivo {nombre_archivo} no tiene columnas Nivel o Elementos")
                continue
            
            # Filtrar por FC_CON_TRISEMANAL = 'Semana 01' por defecto
            if "FC_CON_TRISEMANAL" in dfw.columns:
                dfw = dfw[dfw["FC_CON_TRISEMANAL"] == "Semana 01"]
            
            # Agrupar por Nivel y Elementos
            resumen = dfw.groupby(["Nivel", "Elementos"])["VolumenHA"].sum().reset_index()
            resumen["Fecha"] = fecha
            lista_df.append(resumen)
            
        except Exception as e:
            nombre_archivo = f if use_local_files else f['name']
            st.error(f"Error procesando archivo {nombre_archivo}: {e}")
            continue
    
    if not lista_df:
        st.warning("No hay datos para la comparaci√≥n trisemanal")
        return
    
    # Unir todos los resultados
    df_semana = pd.concat(lista_df, ignore_index=True)
    
    # Crear tabla pivot para comparaci√≥n
    pivot_trisemanal = df_semana.pivot_table(
        values="VolumenHA",
        index=["Nivel", "Elementos"],
        columns="Fecha",
        aggfunc="sum",
        fill_value=0
    ).reset_index()
    
    # Calcular diferencia entre las dos semanas
    fechas = sorted(df_semana["Fecha"].unique())
    if len(fechas) == 2:
        col_actual = fechas[1]
        col_anterior = fechas[0]
        if col_actual in pivot_trisemanal.columns and col_anterior in pivot_trisemanal.columns:
            pivot_trisemanal["Diferencia"] = pivot_trisemanal[col_actual] - pivot_trisemanal[col_anterior]
    
    # Calcular totales
    numeric_cols = pivot_trisemanal.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        pivot_trisemanal["Total"] = pivot_trisemanal[numeric_cols].sum(axis=1)
        if pivot_trisemanal["Total"].sum() > 0:
            pivot_trisemanal["% Avance"] = (pivot_trisemanal["Total"] / pivot_trisemanal["Total"].sum() * 100).round(2)
    
    # Formatear columnas num√©ricas
    for col in pivot_trisemanal.select_dtypes(include=[np.number]).columns:
        pivot_trisemanal[col] = pivot_trisemanal[col].round(2)
    
    # Ordenar por Nivel y Elemento para jerarqu√≠a visual
    pivot_trisemanal = pivot_trisemanal.sort_values(["Nivel", "Elementos"]).reset_index(drop=True)
    
    # Mostrar tabla
    st.subheader("Comparaci√≥n Trisemanal")
    
    # Agregar filtros
    col1, col2 = st.columns(2)
    
    with col1:
        try:
            niveles_raw = df_semana["Nivel"].dropna().unique()
            niveles_clean = [str(n).strip() for n in niveles_raw if str(n).strip()]
            niveles = ["Todos"] + sorted(niveles_clean)
            nivel_seleccionado = st.selectbox("Filtrar por Nivel:", niveles, key="trisemanal_nivel")
        except Exception as e:
            st.error(f"Error al cargar niveles: {e}")
            nivel_seleccionado = "Todos"
    
    with col2:
        try:
            elementos_raw = df_semana["Elementos"].dropna().unique()
            elementos_clean = [str(e).strip() for e in elementos_raw if str(e).strip()]
            elementos = ["Todos"] + sorted(elementos_clean)
            elemento_seleccionado = st.selectbox("Filtrar por Elemento:", elementos, key="trisemanal_elemento")
        except Exception as e:
            st.error(f"Error al cargar elementos: {e}")
            elemento_seleccionado = "Todos"
    
    # Aplicar filtros
    df_filtrado_tabla = pivot_trisemanal.copy()
    try:
        if nivel_seleccionado != "Todos":
            df_filtrado_tabla = df_filtrado_tabla[df_filtrado_tabla["Nivel"] == nivel_seleccionado]
        if elemento_seleccionado != "Todos":
            df_filtrado_tabla = df_filtrado_tabla[df_filtrado_tabla["Elementos"] == elemento_seleccionado]
    except Exception as e:
        st.error(f"Error al aplicar filtros: {e}")
    
    # Mostrar tabla con jerarqu√≠as expandibles
    try:
        # Crear configuraci√≥n de columnas din√°mica
        column_config = {
            "Nivel": st.column_config.TextColumn("Nivel", width="medium"),
            "Elementos": st.column_config.TextColumn("Elementos", width="large"),
        }
        
        # Agregar columnas de fechas
        for col in df_filtrado_tabla.columns:
            if col not in ["Nivel", "Elementos", "Total", "% Avance", "Diferencia"]:
                column_config[col] = st.column_config.NumberColumn(col, format="%.2f")
        
        # Agregar columnas especiales
        if "Diferencia" in df_filtrado_tabla.columns:
            column_config["Diferencia"] = st.column_config.NumberColumn("Diferencia", format="%.2f")
        if "Total" in df_filtrado_tabla.columns:
            column_config["Total"] = st.column_config.NumberColumn("Total", format="%.2f")
        if "% Avance" in df_filtrado_tabla.columns:
            column_config["% Avance"] = st.column_config.NumberColumn("% Avance", format="%.2f%%")
        
        st.dataframe(
            df_filtrado_tabla,
            use_container_width=True,
            hide_index=True,
            column_config=column_config
        )
    except Exception as e:
        st.error(f"Error al mostrar tabla: {e}")
        st.dataframe(df_filtrado_tabla, use_container_width=True)
    
    # Mostrar resumen de diferencias
    if "Diferencia" in df_filtrado_tabla.columns:
        st.subheader("Resumen de Diferencias")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Diferencia Total", f"{df_filtrado_tabla['Diferencia'].sum():.2f}")
        
        with col2:
            positivas = df_filtrado_tabla[df_filtrado_tabla['Diferencia'] > 0]['Diferencia'].sum()
            st.metric("Diferencias Positivas", f"{positivas:.2f}")
        
        with col3:
            negativas = df_filtrado_tabla[df_filtrado_tabla['Diferencia'] < 0]['Diferencia'].sum()
            st.metric("Diferencias Negativas", f"{negativas:.2f}")
    
    # Mostrar m√©tricas adicionales
    if not df_filtrado_tabla.empty and "Total" in df_filtrado_tabla.columns:
        try:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                total_sum = df_filtrado_tabla['Total'].sum()
                st.metric("Total General", f"{total_sum:.2f}")
            
            with col2:
                if len(fechas) >= 2:
                    ultima_fecha = fechas[-1]
                    if ultima_fecha in df_filtrado_tabla.columns:
                        ultimo_total = df_filtrado_tabla[ultima_fecha].sum()
                        st.metric(f"Total {ultima_fecha.strftime('%d/%m/%Y')}", f"{ultimo_total:.2f}")
            
            with col3:
                if len(fechas) >= 2:
                    primera_fecha = fechas[0]
                    if primera_fecha in df_filtrado_tabla.columns:
                        primer_total = df_filtrado_tabla[primera_fecha].sum()
                        st.metric(f"Total {primera_fecha.strftime('%d/%m/%Y')}", f"{primer_total:.2f}")
                        
        except Exception as e:
            st.error(f"Error al calcular m√©tricas: {e}")

@st.cache_data(ttl=3600)  # Cache por 1 hora
def cargar_datos_local():
    """Carga el archivo AO_GENERAL.txt desde archivo local"""
    try:
        local_file = "AO_GENERAL.txt"
        if os.path.exists(local_file):
            df = pd.read_csv(local_file, sep="\t", header=1, dtype=str)
            df = df.dropna(how="all")
            
            # Limpiar columnas
            df = df.rename(columns=lambda x: x.strip().replace('"', ''))
            
            # Convertir tipos de datos
            for col in ["VolumenHA", "AreaMoldaje", "Cuantia"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col].str.replace(",", ".", regex=False), errors='coerce')
            
            if "FC_CON_FECHA EJECUCION" in df.columns:
                df["FC_CON_FECHA EJECUCION"] = pd.to_datetime(
                    df["FC_CON_FECHA EJECUCION"], 
                    dayfirst=True, 
                    errors='coerce'
                )
            
            st.success("‚úÖ Datos cargados desde archivo local")
            return df
        else:
            st.error("‚ùå No se encontr√≥ el archivo local AO_GENERAL.txt")
            return None
    except Exception as e:
        st.error(f"‚ùå Error al cargar archivo local: {e}")
        return None

def cargar_archivos_semanales_local():
    """Carga archivos semanales desde la carpeta local"""
    try:
        carpeta = "REPORTE SEMANAL"
        if not os.path.exists(carpeta):
            st.error(f"‚ùå Carpeta '{carpeta}' no encontrada")
            return [], None
        
        archivos = []
        for filename in os.listdir(carpeta):
            if filename.endswith('.txt') and 'AO_GENERAL' in filename:
                # Extraer fecha del nombre del archivo
                fecha_str = filename.split('_')[0]  # Tomar la primera parte antes del primer _
                
                # Intentar diferentes formatos de fecha
                fecha = None
                for formato in ['%d-%m-%Y', '%d-%m-%y', '%Y-%m-%d']:
                    try:
                        fecha = datetime.strptime(fecha_str, formato)
                        break
                    except ValueError:
                        continue
                
                if fecha:
                    archivos.append((filename, fecha))
        
        # Ordenar por fecha
        archivos.sort(key=lambda x: x[1])
        
        if not archivos:
            st.warning("‚ùå No se encontraron archivos semanales en la carpeta local")
            return [], None
        
        return archivos, None  # None indica que es local
        
    except Exception as e:
        st.error(f"‚ùå Error al cargar archivos locales: {e}")
        return [], None

def leer_archivo_local(filepath):
    """Lee directamente un archivo local"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        from io import StringIO
        return StringIO(content)
    except Exception as e:
        st.error(f"Error leyendo archivo local {filepath}: {e}")
        return None

# Funci√≥n principal
def main():
    # Configuraci√≥n en sidebar
    st.sidebar.header("‚öôÔ∏è Configuraci√≥n")
    
    # Opci√≥n para forzar uso de archivos locales
    use_local_files = st.sidebar.checkbox(
        "üìÅ Usar archivos locales (ignorar Google Drive)",
        key="main_local_checkbox",
        help="Marca esta opci√≥n si quieres usar archivos locales en lugar de Google Drive"
    )
    
    if use_local_files:
        st.sidebar.info("üìÅ Modo archivos locales activado")
        # Verificar archivos locales
        if not os.path.exists("AO_GENERAL.txt"):
            st.sidebar.error("‚ùå No se encontr√≥ AO_GENERAL.txt")
        else:
            st.sidebar.success("‚úÖ AO_GENERAL.txt encontrado")
        
        if not os.path.exists("REPORTE SEMANAL"):
            st.sidebar.warning("‚ö†Ô∏è No se encontr√≥ carpeta REPORTE SEMANAL")
        else:
            weekly_files = [f for f in os.listdir("REPORTE SEMANAL") if f.endswith('_AO_GENERAL.txt')]
            if weekly_files:
                st.sidebar.success(f"‚úÖ {len(weekly_files)} archivos semanales encontrados")
            else:
                st.sidebar.warning("‚ö†Ô∏è No se encontraron archivos semanales")
    
    # Debug: Verificar que los secretos est√©n configurados (solo si no se usan archivos locales)
    if not use_local_files:
        try:
            # Verificar que existan los secretos necesarios
            required_secrets = ["GOOGLE_CREDENTIALS", "FILE_ID_GENERAL", "FOLDER_ID_SEMANAL"]
            missing_secrets = []
            
            for secret in required_secrets:
                if secret not in st.secrets:
                    missing_secrets.append(secret)
            
            if missing_secrets:
                st.error(f"Faltan los siguientes secretos: {', '.join(missing_secrets)}")
                st.info("Configura estos secretos en Streamlit Cloud > Settings > Secrets")
                st.info("üí° Alternativamente, puedes usar archivos locales:")
                st.info("- Coloca AO_GENERAL.txt en la ra√≠z del proyecto")
                st.info("- Coloca los archivos semanales en la carpeta 'REPORTE SEMANAL'")
                return
            
            st.success("‚úÖ Todos los secretos configurados")
        
        except Exception as e:
            st.error(f"Error al verificar configuraci√≥n: {e}")
            return
    
    # Verificar archivos locales como alternativa
    local_files_exist = False
    if os.path.exists("AO_GENERAL.txt"):
        st.info("üìÅ Archivo local AO_GENERAL.txt encontrado")
        local_files_exist = True
    
    if os.path.exists("REPORTE SEMANAL"):
        weekly_files = [f for f in os.listdir("REPORTE SEMANAL") if f.endswith('_AO_GENERAL.txt')]
        if weekly_files:
            st.info(f"üìÅ Carpeta local REPORTE SEMANAL encontrada con {len(weekly_files)} archivos")
            local_files_exist = True
    
    # Cargar datos
    if use_local_files:
        # Forzar uso de archivos locales
        df = cargar_datos_local()
    else:
        df = cargar_datos()
    
    if df is None:
        st.error("No se pudieron cargar los datos")
        if local_files_exist:
            st.info("üí° Se detectaron archivos locales. La aplicaci√≥n intentar√° usarlos autom√°ticamente.")
        else:
            st.info("üí° Para usar archivos locales:")
            st.info("1. Coloca AO_GENERAL.txt en la ra√≠z del proyecto")
            st.info("2. Coloca los archivos semanales en la carpeta 'REPORTE SEMANAL'")
        return
    
    # Debug: Mostrar informaci√≥n sobre los datos
    st.success(f"‚úÖ Datos cargados exitosamente: {len(df)} filas")
    st.info(f"Columnas disponibles: {', '.join(df.columns.tolist())}")
    
    # Mostrar primeras filas para debug
    with st.expander("üîç Ver datos cargados (Debug)"):
        st.write("Primeras 5 filas:")
        st.dataframe(df.head(), use_container_width=True)
        
        st.write("Informaci√≥n del DataFrame:")
        st.write(f"- Filas: {len(df)}")
        st.write(f"- Columnas: {len(df.columns)}")
        st.write(f"- Columnas con datos: {df.columns.tolist()}")
    
    # Crear pesta√±as
    tabs = st.tabs([
        "üìä Avance General",
        "üìà Avance Semanal",
        "üîÑ Avance Trisemanal"
    ])

    # Pesta√±a Avance General
    with tabs[0]:
        st.header("üìä Avance General de Obra")
        st.markdown("### Hormigones")
        crear_tabla_interactiva(df, "Avance de Hormigones", "VolumenHA", tab_key="hormigones_general")
        st.markdown("### Moldajes")
        crear_tabla_interactiva(df, "Avance de Moldajes", "AreaMoldaje", tab_key="moldajes_general")
        st.markdown("### Enfierraduras")
        crear_tabla_interactiva(df, "Avance de Enfierraduras", "Cuantia", tab_key="enfierraduras_general")

    # Pesta√±a Avance Semanal
    with tabs[1]:
        mostrar_avance_semanal(use_local_files)

    # Pesta√±a Avance Trisemanal
    with tabs[2]:
        mostrar_trisemanal(use_local_files)

# Ejecutar aplicaci√≥n
if __name__ == "__main__":
    main() 